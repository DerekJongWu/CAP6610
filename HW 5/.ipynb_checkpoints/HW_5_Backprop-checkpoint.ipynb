{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n",
      "[ 0. -1.] [[0.94748975]]\n",
      "[0. 1.] [[0.9563562]]\n",
      "[1. 0.] [[0.09833899]]\n",
      "[-1.  0.] [[0.20897367]]\n",
      "[ 0.74981904 -2.4598148 ] [[0.94748943]]\n",
      "[1.31929701 0.61136545] [[0.26955226]]\n",
      "[-0.441491   -1.13908785] [[0.15406234]]\n",
      "[1.98495133 0.02693565] [[0.09836224]]\n",
      "[ 0.35880669 -1.61454139] [[0.94748939]]\n",
      "[-0.00993354 -1.38947805] [[0.94748943]]\n",
      "[ 1.22956251 -0.25388263] [[0.09833899]]\n",
      "[-1.69610226 -0.95617171] [[0.20897367]]\n",
      "[1.15563935 0.24133141] [[0.26928078]]\n",
      "[0.27677085 1.04420738] [[0.9563562]]\n",
      "[-1.0765125   0.11634145] [[0.20897367]]\n",
      "[2.71679361 0.55569945] [[0.26955226]]\n",
      "[-0.04361956  1.2226314 ] [[0.9563562]]\n",
      "[0.04559882 1.38058694] [[0.9563562]]\n",
      "[-0.12038412 -1.36735395] [[0.94748943]]\n",
      "[-1.62232121 -0.4290878 ] [[0.20897367]]\n",
      "[0.88797378 1.15788279] [[0.95636183]]\n",
      "[-0.48592231  1.03849998] [[0.95635609]]\n",
      "[-0.41224719  1.32834536] [[0.9563562]]\n",
      "[0.46028561 2.07686269] [[0.9563562]]\n",
      "[-0.6777886   1.92452674] [[0.9563562]]\n",
      "[-0.70173425 -1.61013768] [[0.99267984]]\n",
      "[-0.62091514  1.1985025 ] [[0.95635595]]\n",
      "[-1.30380298  0.21220811] [[0.20897367]]\n",
      "[ 1.44903167 -0.49834163] [[0.09833899]]\n",
      "[-2.05694781 -0.8534615 ] [[0.20897367]]\n",
      "[-1.19528179 -0.82327108] [[0.20897367]]\n",
      "[ 2.08880579 -0.92323656] [[0.09833899]]\n",
      "[-0.36440042 -1.24660636] [[0.95122735]]\n",
      "[-0.84751893  1.20836238] [[0.95598582]]\n",
      "[-1.55546873 -0.25248386] [[0.20897367]]\n",
      "[-1.38057624  0.29332976] [[0.20897589]]\n",
      "[-1.03479122  0.51681073] [[0.07196846]]\n",
      "[ 1.07428154 -0.18478572] [[0.09833899]]\n",
      "[-1.64629599  0.57650839] [[0.07196825]]\n",
      "[-0.32355154  1.13931454] [[0.9563562]]\n",
      "[1.95658763 0.96826119] [[0.26955226]]\n",
      "[0.22481133 1.00050563] [[0.9563562]]\n",
      "[-1.11931743 -0.39027183] [[0.20897367]]\n",
      "[ 0.98994064 -1.71200744] [[0.09751488]]\n",
      "[-1.35598886  0.55051514] [[0.0719683]]\n",
      "[-1.3202005  -0.31868241] [[0.20897367]]\n",
      "[ 1.19649267 -0.19113763] [[0.09833899]]\n",
      "[-0.27070891 -1.37149846] [[0.94749069]]\n",
      "[-0.69719977 -1.3066631 ] [[0.20863662]]\n",
      "[-0.2656828   1.20349004] [[0.9563562]]\n",
      "[ 0.50877521 -1.13532086] [[0.21293686]]\n",
      "[ 0.22424532 -2.77436075] [[0.94748943]]\n",
      "[ 1.15216685 -0.90460281] [[0.09833899]]\n",
      "[-1.1424233   0.23544239] [[0.20897373]]\n",
      "[-0.92059782 -1.97710183] [[0.97450985]]\n",
      "[-0.27165506 -2.03262146] [[0.94748943]]\n",
      "[-0.67414375 -1.53382538] [[0.82106551]]\n",
      "[-0.62383863  1.04835921] [[0.95634729]]\n",
      "[1.52023325 0.3612511 ] [[0.26955226]]\n",
      "[-0.40782776 -1.57437038] [[0.94749055]]\n",
      "[-1.6417257  -0.64924401] [[0.20897367]]\n",
      "[1.11608981 0.1931333 ] [[0.22237783]]\n",
      "[-1.00802767 -0.9987904 ] [[0.20897367]]\n",
      "[-1.62488744  0.33071456] [[0.20898286]]\n",
      "[-0.84571993 -2.13608229] [[0.94750023]]\n",
      "[ 2.33610773 -0.24702855] [[0.09833899]]\n",
      "[-0.00656781  1.17919049] [[0.9563562]]\n",
      "[0.09771117 2.77625415] [[0.9563562]]\n",
      "[ 1.17274053 -0.92680656] [[0.09833899]]\n",
      "[ 2.76283287 -0.3121804 ] [[0.09833899]]\n",
      "[-1.20256214  0.9864285 ] [[0.07201336]]\n",
      "[0.02609649 1.92950146] [[0.9563562]]\n",
      "[2.16471728 0.25147816] [[0.26955225]]\n",
      "[0.25806093 1.20683384] [[0.9563562]]\n",
      "[-0.55415736 -1.03504   ] [[0.20892673]]\n",
      "[ 0.81816145 -1.3603421 ] [[0.09820905]]\n",
      "[1.36968893 0.93790286] [[0.26955241]]\n",
      "[ 1.59782262 -0.03105124] [[0.09833899]]\n",
      "[-1.39659012  0.09651945] [[0.20897367]]\n",
      "[ 1.05133514 -0.7992288 ] [[0.09833899]]\n",
      "[ 1.56168857 -0.23138605] [[0.09833899]]\n",
      "[-1.65934507  0.69074607] [[0.07196847]]\n",
      "[-1.53042002 -0.03005147] [[0.20897367]]\n",
      "[-0.35423166  1.7897203 ] [[0.9563562]]\n",
      "[-1.37786489 -0.16837125] [[0.20897367]]\n",
      "[1.03882261 0.58493425] [[0.26955994]]\n",
      "[-0.60810678  1.21096354] [[0.95635608]]\n",
      "[-1.45111348  0.41424481] [[0.00047085]]\n",
      "[1.10140015 0.58448923] [[0.26955272]]\n",
      "[-1.92203091 -0.4736467 ] [[0.20897367]]\n",
      "[-0.31952409  2.01249327] [[0.9563562]]\n",
      "[ 1.15373105 -0.18901844] [[0.09833899]]\n",
      "[0.64768791 1.26777685] [[0.9563562]]\n",
      "[0.21790903 1.0765002 ] [[0.9563562]]\n",
      "[-0.48387526 -1.09145141] [[0.20316715]]\n",
      "[ 0.10751311 -1.04172532] [[0.94748487]]\n",
      "[-0.34518096 -2.20410805] [[0.94748943]]\n",
      "[ 0.26958638 -1.75507785] [[0.94748943]]\n",
      "[0.82518095 1.45940061] [[0.9563562]]\n",
      "[ 0.80001577 -1.42115782] [[0.09767623]]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-3*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers, activation='sigmoid'):\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "            self.activation_prime = sigmoid_prime\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "            self.activation_prime = tanh_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        np.random.seed(4)\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.01, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            if k % 10000 == 0: print ('epochs:', k)\n",
    "            \n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "            \n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = a[-1] - y[i]\n",
    "            deltas = [error]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] -= learning_rate * layer.T.dot(delta)\n",
    "\n",
    "    def predict(self, x): \n",
    "        a = np.concatenate((np.array([[1]]), np.array([x])), axis=1)      \n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,2,1])\n",
    "\n",
    "    X = np.array([[0, -1],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [-1, 0]])\n",
    "\n",
    "    y = np.array([1, 1, 0, 0])\n",
    "    class1 = 2\n",
    "    class2 = 2\n",
    "    while len(X) < 100: \n",
    "        val = np.random.normal(size = 2)\n",
    "        val1 = abs(val[0])\n",
    "        val2 = abs(val[1])\n",
    "        if ((val1 > 1) and (val2 < 1)): \n",
    "            if class1 < 50: \n",
    "                label = 0\n",
    "                y = np.append(y,label)\n",
    "                val = [val]\n",
    "                X = np.append(X,val,axis=0)\n",
    "                class1 +=1\n",
    "            else: continue\n",
    "        if ((val1 < 1) and (val2 > 1)):\n",
    "            if class2 < 50: \n",
    "                label = 1 \n",
    "                y = np.append(y,label)\n",
    "                val = [val]\n",
    "                X = np.append(X,val,axis=0)\n",
    "                class2 +=1\n",
    "            else: continue\n",
    "        else: \n",
    "            continue\n",
    "\n",
    "    nn.fit(X, y)\n",
    "    #print(nn.weights)\n",
    "    index = 0 \n",
    "    misclassified = 0\n",
    "    wrong = 0 \n",
    "    for e in X:\n",
    "        out = nn.predict(e)\n",
    "        comp = y[index]\n",
    "        print(e, nn.predict(e))\n",
    "        if abs(out-comp) > 0.5: \n",
    "            misclassified += 1\n",
    "            index += 1\n",
    "        else:\n",
    "            index += 1\n",
    "            continue\n",
    "    print(misclassified)\n",
    "    #print(nn.weights)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
